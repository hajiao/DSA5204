# -*- coding: utf-8 -*-
"""DSA5204_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18prkDpKxiVKiEAM4sHVkugMJAarZHWbt
"""

# from google.colab import drive
import os
# drive.mount('/content/drive')

import tensorflow as tf

#import necessary packages

from tensorflow import keras
import keras_cv
import numpy as np
import re
import copy
import matplotlib.pyplot as plt
import functools

"""Model Construction"""

class SE(keras.layers.Layer):
  ''' 
  Squeeze and Excite layer (block)
  used to adding a weight to each channel
  to avoid exposure of output, using sigmoid on the weights making them positive and sum 1.
  '''
  def __init__(self,filters,ratio=.25,act='relu'):
    super().__init__()
    reduced_filters=max(1,int(filters*ratio))
    self.squeeze=keras.layers.Conv2D(
      reduced_filters,
      kernel_size=1,
      strides=1,
      padding='same',
      activation=act
  )
    self.excite=keras.layers.Conv2D(
      filters,
      kernel_size=1,
      strides=1,
      padding='same',
      activation='sigmoid',
  )
    self.globalavgpool=keras.layers.GlobalAveragePooling2D(keepdims=True)
  @tf.function
  def call(self,inputx):
    x=self.globalavgpool(inputx)
    x=self.squeeze(x)
    x=self.excite(x)
    return inputx*x

'''
  Convolution-BatchNormalization-Activation (CBA) layer (block)
  Because there is research showing that activation after batch normalization performs better
  (at least in CV tasks is valid)
  EfficientNetV2 model also adopt this concept
'''
class CBA2D_layer(keras.layers.Conv2D):
  def __init__(self,*args,kernel_initializer=keras.initializers.HeNormal(seed=5204),use_bias=False,**kwargs):
    super().__init__(*args,
                     kernel_initializer=kernel_initializer,
                     use_bias=use_bias,
                     **kwargs)
    self.act=self.activation
    self.activation=None
    self.bn=keras.layers.BatchNormalization()
  @tf.function
  def call(self, input, training):
    x=super().call(input)
    x=self.bn(x,training)
    x=self.act(x)
    return x
class DepthwiseCBA2D_layer(keras.layers.DepthwiseConv2D):
  def __init__(self,*args,depthwise_initializer=keras.initializers.VarianceScaling(
    scale=2.0, mode="fan_out", distribution="untruncated_normal", seed=5204),
                use_bias=False,**kwargs):
    super().__init__(*args,
                     depthwise_initializer=depthwise_initializer,
                     use_bias=use_bias,
                     **kwargs)
    self.act=self.activation
    self.activation=None
    self.bn=keras.layers.BatchNormalization()
  @tf.function
  def call(self, input, training):
    x=super().call(input)
    x=self.bn(x,training)
    x=self.act(x)
    return x

'''
    deprecated
  drop the whole channel by possibility of 1-survival_probabillity
  to achieve so-call 'stachastic depth'
'''
class drop_connect_layer(keras.layers.Layer):
  def __init__(self,survival_prob):
    super().__init__()
    self.survival_prob=survival_prob
  @tf.function
  def call(self,inputs,training):
    """Drop the entire conv with given survival probability."""
    # "Deep Networks with Stochastic Depth", https://arxiv.org/pdf/1603.09382.pdf
    def train_fn():
      # Compute tensor.
      batch_size = tf.shape(inputs)[0]
      binary_tensor = tf.cast(tf.random.uniform([batch_size,1,1,1])<self.survival_prob,dtype=inputs.dtype)
      # Unlike conventional way that multiply survival_prob at test time, here we
      # divide survival_prob at training time, such that no addition compute is
      # needed at test time.
      output = inputs / self.survival_prob * binary_tensor
      return output
    # return tf.cond(tf.cast(training,tf.bool),train_fn,lambda: inputs)
    if training:
      return train_fn()
    return inputs

'''
  The core block of the EfficientNet
  using res-block type architecture in repeated layers
  using channel-wise FC ((1x1) kernel) to expand the features
  then use depthwise Conv to get better performance than original Conv
  using Squeeze-and-Excite layer to weighted each features 
  then project to the original feature  
  could add dropout layer after SE to add regularization
'''
class MBC_Block(keras.layers.Layer):
  def __init__(self,act,args,survival_prob=1):
    super().__init__()
    filters=int(args['input_filters']*args['expand_ratio'])
    self.expandblock=None
    if args['expand_ratio']!=1:
      self.expandblock=CBA2D_layer(
            filters=filters,
            kernel_size=1,
            strides=1,
            padding='same',
            use_bias=False,
            activation=act
          )
    self.depwiseblock=DepthwiseCBA2D_layer(
      kernel_size=args['kernel_size'],
      strides=args['strides'],
      padding='same',
      use_bias=False,
      activation=act
    )
    self.conv_dropout=None
    if 'conv_dropout' in args.keys():
      if args['conv_dropout'] and args['expand_ratio'] > 1:
        self.conv_dropout=keras.layers.Dropout(args['conv_dropout'])
    if args['se_ratio']:
      self.se=SE(filters,args['se_ratio']/args['expand_ratio'],act)
    else:
      self.se=None
    self.outputblock=CBA2D_layer(
        filters=args['output_filters'],
        kernel_size=1,
        strides=1,
        padding='same',
        use_bias=False,
        activation=None
    )
    self.residual_dropout=None
    self.residual=None
    if args['strides']==1 and args['input_filters']==args['output_filters']:
      if 1-survival_prob:
        self.residual_dropout=keras.layers.SpatialDropout2D(1-survival_prob)
      self.residual=self._residual
      
  @tf.function
  def _residual(self,x,input):
    if self.residual_dropout:
      x=self.residual_dropout(x)
    return tf.add(x,input)
  @tf.function
  def call(self,input):
    x=input
    if self.expandblock:
      x=self.expandblock(x)
    x=self.depwiseblock(x)
    if self.conv_dropout:
      x=self.conv_dropout(x)
    if self.se:
      x=self.se(x)
    x=self.outputblock(x)
    if self.residual:
      x=self.residual(x,input)
    return x


'''
  Another Core block of EfficientNetV2
  Using Convolution layer replacing the channel-wise FC and Depthwise Conv to reduce computation cost
  it finds that Depthwise convolutions are slow in early layers but effective in later stages
'''
class FusedMBC_Block(keras.layers.Layer):
  def __init__(self,act,args,survival_prob=1):
    super().__init__()
    filters=int(args['input_filters']*args['expand_ratio'])
    self.expandblock=None
    if args['expand_ratio']!=1:
      self.expandblock=CBA2D_layer(
            filters=filters,
            kernel_size=args['kernel_size'],
            strides=args['strides'],
            padding='same',
            use_bias=False,
            activation=act
          )
    self.conv_dropout=None
    if 'conv_dropout' in args.keys():
      if args['conv_dropout'] and args['expand_ratio'] > 1:
        self.conv_dropout=keras.layers.Dropout(args['conv_dropout'])
    if args['se_ratio']:
      self.se=SE(filters,args['se_ratio']/args['expand_ratio'],act)
    else:
      self.se=None
    self.outputblock=CBA2D_layer(
        filters=args['output_filters'],
        kernel_size=args['kernel_size'] if args['expand_ratio'] == 1 else 1,
        strides=args['strides'] if args['expand_ratio'] == 1 else 1,
        padding='same',
        use_bias=False,
        activation=act if args['expand_ratio'] == 1 else None
    )
    self.residual_dropout=None
    self.residual=None
    if args['strides']==1 and args['input_filters']==args['output_filters']:
      if 1-survival_prob:
        self.residual_dropout=keras.layers.SpatialDropout2D(1-survival_prob)
      self.residual=self._residual
      
  @tf.function
  def _residual(self,x,input):
    if self.residual_dropout:
      x=self.residual_dropout(x)
    return tf.add(x,input)
  @tf.function
  def call(self,input):
    x=input
    if self.expandblock:
      x=self.expandblock(x)
    if self.conv_dropout:
      x=self.conv_dropout(x)
    if self.se:
      x=self.se(x)
    x=self.outputblock(x)
    if self.residual:
      x=self.residual(x,input)
    return x

'''
The backbone of EfficientNetV2 model
consistence:  Stem + multiple MBConv/Fused MBConv layers + head
Stem layer is just a CBA block with 3x3 kernel and strides 2
Head layer is a channel-wise FC layer and a GlobalAverage layer fusing all channels in one 
  to reduce computation cost of further FC layers
'''
class EfficientV2base(keras.Model):
  def __init__(self,act,block_cfgs,head_filters=1280,survival_prob=0.8,dropout=None,pooling='avg'):
    super().__init__()
    self.stem=CBA2D_layer(
        filters=block_cfgs[0]['input_filters'],
        kernel_size=3,
        strides=2,
        padding='same',
        use_bias=False,
        activation=act)
    self.blocks=[]
    for idx,block_cfg in enumerate(block_cfgs):
      if survival_prob:
        drop_rate = 1.0 - survival_prob
        survival_prob = 1.0 - drop_rate * float(idx) / len(block_cfgs)
      if block_cfg['conv_type']:
        self.blocks.append(FusedMBC_Block(act,block_cfg,survival_prob))
      else:
        self.blocks.append(MBC_Block(act,block_cfg,survival_prob))
    self.head_block=CBA2D_layer(filters=head_filters,
        kernel_size=1,
        strides=1,
        padding='same',
        use_bias=False,
        activation=act)
    if pooling=='avg':
      self.head_pool=keras.layers.GlobalAveragePooling2D()
    elif pooling=='max':
      self.head_pool=keras.layers.GlobalMaxPool2D()
    else:
      self.head_pool=keras.layers.Flatten()
    self.dropout=None
    if dropout and dropout>0 and dropout<1:
      self.dropout=keras.layers.Dropout(dropout)
  @tf.function
  def head(self,x):
    x=self.head_block(x)
    x=self.head_pool(x)
    if self.dropout:
      x=self.dropout(x)
    return x
  @tf.function
  def call(self,x):
    x=self.stem(x)
    for block in self.blocks:
      x=block(x)
    x=self.head(x)
    return x

#for larger models, unused
def round_filters(filters, mconfig, skip=False):
  """Round number of filters based on depth multiplier."""
  multiplier = mconfig.width_coefficient
  divisor = mconfig.depth_divisor
  min_depth = mconfig.min_depth
  if skip or not multiplier:
    return filters

  filters *= multiplier
  min_depth = min_depth or divisor
  new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)
  return int(new_filters)
def round_repeats(repeats, multiplier, skip=False):
  """Round number of filters based on depth multiplier."""
  if skip or not multiplier:
    return repeats
  return int(np.ceil(multiplier * repeats))

'''
function that decodes the blocks config of EfficientNetV2 model
It is just a copy
'''
def decode_block_cfg(s):
  ops = s.split('_')
  options = {}
  for op in ops:
    splits = re.split(r'(\d.*)', op)
    if len(splits) >= 2:
      key, value = splits[:2]
      options[key] = value
  return dict(
      kernel_size=int(options['k']),
      num_repeat=int(options['r']),
      input_filters=int(options['i']),
      output_filters=int(options['o']),
      expand_ratio=int(options['e']),
      se_ratio=float(options['se']) if 'se' in options else None,
      strides=int(options['s']),
      conv_type=int(options['c']) if 'c' in options else 0,
  )

'''
decode the blocks config above for latter model construction 
'''
def decode_cfgs(cfgs):
  block_cfgs=[]
  for s in cfgs:
    block_cfg=decode_block_cfg(s)
    block_cfgs.append(copy.copy(block_cfg))
    if block_cfg['num_repeat']>1:
      block_cfg['input_filters']=block_cfg['output_filters']
      block_cfg['strides']=1
    for _ in range(block_cfg['num_repeat']-1):
      block_cfgs.append(copy.copy(block_cfg))
  return block_cfgs


'''
Custom loss: adding l2 regulation term of all trainable weights 
Modified from origin code
'''
class add_l2_loss(keras.losses.Loss):
  def __init__(self,loss,weight_decay,model):
    super().__init__()
    self.loss=loss
    self.weight_decay=weight_decay
    self.model=model
    regex=r'.*(kernel|weight):0$'
    self.var_match=re.compile(regex)
  @tf.function
  def call(self,y_true,y_pred):
    loss=self.loss(y_true,y_pred)
    return loss+self.weight_decay*tf.add_n([tf.nn.l2_loss(v)
        for v in self.model.trainable_variables
        if self.var_match.match(v.name)])

'''
Warmup Learning Rate Schedule:
linearly increase the learning rate from 0 to target initial value, increase after each batch to make it more smooth
Modified from origin code
'''
class WarmupLearningRateSchedule(
    keras.optimizers.schedules.LearningRateSchedule):
  """Provides a variety of learning rate decay schedules with warm up."""

  def __init__(self,
        initial_lr,
        steps_per_epoch,
        after_warmup_schedule=None,
        warmup_epochs=5,
        minimal_lr=0,
        last_epoch=0):
    super(WarmupLearningRateSchedule, self).__init__()
    self.initial_lr = initial_lr
    self.steps_per_epoch = steps_per_epoch
    self.warmup_epochs = warmup_epochs
    self.warmup_steps = int(warmup_epochs * steps_per_epoch)
    self.minimal_lr = minimal_lr
    if after_warmup_schedule:
      self.after_warmup_schedule=after_warmup_schedule
    else:
      self.after_warmup_schedule=lambda step: self.initial_lr
    self.last_epoch=last_epoch

  def __call__(self, step):
    step=tf.add(step,self.last_epoch*self.steps_per_epoch)
    if self.minimal_lr:
      lr = tf.math.maximum(lr, self.minimal_lr)

    warmup_lr = (
        self.initial_lr * tf.cast(step, tf.float32) /
        tf.cast(self.warmup_steps, tf.float32))
    lr = tf.cond(step < self.warmup_steps, lambda: warmup_lr, lambda: self.after_warmup_schedule(step-self.warmup_steps))
    return lr


"""Random Augmentation (without mixup,cutmix)
https://keras.io/guides/keras_cv/cut_mix_mix_up_and_rand_augment/
no used
"""
def level_to_arg(level,cutout_const,translate_const):
  return {
      'AutoContrast': 0,
      'Equalize': 0,
      'Invert': 0,
      'Rotate': (level/_MAX_LEVEL) /6.,
      'Posterize':  int((level/_MAX_LEVEL) * 4),
      'Solarize': level/_MAX_LEVEL,
      'SolarizeAdd': int((level/_MAX_LEVEL) * 110),
      'Color': level/_MAX_LEVEL * 1.8 + 0.1,
      'Contrast': (level/_MAX_LEVEL) * 1.8 + 0.1,
      'Brightness': (level/_MAX_LEVEL) * 1.8 + 0.1,
      'Sharpness': (level/_MAX_LEVEL) * 1.8 + 0.1,
      'ShearX': (level/_MAX_LEVEL) * 0.3,
      'ShearY': (level/_MAX_LEVEL) * 0.3,
      'Cutout': int((level/_MAX_LEVEL) * cutout_const),
      'TranslateX': (level/_MAX_LEVEL) * float(translate_const),
      'TranslateY': (level/_MAX_LEVEL) * float(translate_const),
  }

'''
function to get the augmentation pipeline
type of augmentations are same of used in origin code
it is good for augmentation functions to run under default number range (0, 255)
  since the change of number range is imcompatable for some parameter initializers (maybe is a bug)
'''
def Set_levels(level,ag_per_image=3,ag_rate=1.0,_MAX_LEVEL = 10.):
  factor=level/_MAX_LEVEL
  layers=keras_cv.layers.RandAugment.get_standard_policy(value_range=(0, 255), magnitude=factor, magnitude_stddev=0.3)
  layers+=[keras_cv.layers.RandomRotation(factor/5.),
      keras_cv.layers.Solarization((0,255),dtype='float32'), #invert
      keras_cv.layers.Solarization((0,255),threshold_factor=layers[2].threshold_factor,addition_factor=keras_cv.ConstantFactorSampler(0),dtype='float32'),
      keras_cv.layers.RandomSharpness(factor,(0,255),dtype='float32'),
      keras_cv.layers.RandomCutout((0.1*factor,0.2*factor),(0.1*factor,0.2*factor),dtype='float32'),
      keras_cv.layers.Posterization((0,255),tf.cast(tf.maximum(1,int(factor*4)),tf.uint8))]

  return keras_cv.layers.RandomAugmentationPipeline(layers,ag_per_image,ag_rate)


'''
function to implement augmentation and then scale to (0,1)
'''
def preparing_withscale(samples,ag_pipeline): #image,label
  samples['images']=ag_pipeline(samples['images'])
  return samples

cut_mix = keras_cv.layers.CutMix()
mix_up = keras_cv.layers.MixUp()
def cut_mix_and_mix_up(samples):
    samples = cut_mix(samples, training=True)
    samples = mix_up(samples, training=True)
    return samples
def preprocess_for_model(inputs):
    images, labels = inputs["images"], inputs["labels"]
    images = tf.cast(images, tf.float32)/255.
    return images, labels


''' choosing saved best/last model name'''
def select_saved_model(model_path,choosing_best=True):
  model_list=os.listdir(model_path)
  regex=r'e(?P<epoch>\d*)_l(?P<loss>[\d\.]*).hdf5$'
  var_match=re.compile(regex)
  best_idx=[0,10,0]
  last_idx=[0,10,0]
  for i,m in enumerate(model_list):
    ret=var_match.match(m)
    if best_idx[1]>float(ret['loss']):
      best_idx=[i,float(ret['loss']),int(ret['epoch'])]
    if last_idx[2]<int(ret['epoch']):
      last_idx=[i,float(ret['loss']),int(ret['epoch'])]
  if choosing_best:
    return model_list[best_idx[0]],best_idx[1:]
  else:
    return model_list[last_idx[0]],last_idx[1:]

def checkstage(model_path,stage):
    model_list=os.listdir(model_path)
    regex=f's{stage}-'+r'(\d*)-([\d\.]*).hdf5$'
    var_match=re.compile(regex)
    for i,m in enumerate(model_list):
        ret=var_match.match(m)
        if ret:
            return True
    regex=f's{stage}-final.hdf5$'
    var_match=re.compile(regex)
    for i,m in enumerate(model_list):
        ret=var_match.match(m)
        if ret:
            return True
    return False

'''modified MBC_Block from ConvNeXt:
    less normalization
    less activation
    Moving up conv layer. (inverted bottlenecks)
'''
class Modified_MBC_Block(keras.layers.Layer):
  def __init__(self,act,args,survival_prob=1,layer_scale=1e-6):
    super().__init__()
    filters=int(args['input_filters']*args['expand_ratio'])
    self.expandblock=None
    if args['expand_ratio']!=1:
      self.expandblock=keras.layers.Conv2D(
            filters=filters,
            kernel_size=1,
            strides=1,
            padding='same',
            use_bias=False,
            activation=None
          )
    self.depwiseblock=keras.layers.DepthwiseConv2D(
      kernel_size=args['kernel_size'],
      strides=1,
      padding='same',
      use_bias=False,
      activation=None
    )
    self.conv_dropout=None
    if 'conv_dropout' in args.keys():
      if args['conv_dropout'] and args['expand_ratio'] > 1:
        self.conv_dropout=keras.layers.Dropout(args['conv_dropout'])
    if args['se_ratio']:
      self.se=SE(filters,args['se_ratio']/args['expand_ratio'],act)
    else:
      self.se=None
    self.outputblock=keras.layers.Conv2D(
        filters=args['output_filters'],
        kernel_size=1,
        strides=1,
        padding='same',
        use_bias=False,
        activation=act
    )
    
    self.residual_dropout=None
    self.use_residual=None
    if args['input_filters']==args['output_filters'] or args['strides']>1:
      if 1-survival_prob:
        self.residual_dropout=keras.layers.SpatialDropout2D(1-survival_prob)
      self.use_residual=tf.constant(True,tf.bool)
    
    self.downsample=None
    if args['strides']>1:
        self.downsample_LN=keras.layers.LayerNormalization(epsilon=1e-6)
        self.downsample=keras.layers.Conv2D(
            filters=args['output_filters'],
            kernel_size=2,
            strides=2,
            activation=None
        )
    self.act=self.outputblock.activation
    self.outputblock.activation=None
    self.LN=keras.layers.LayerNormalization(epsilon=1e-6)
    if self.use_residual:
        self.scale=tf.Variable(layer_scale * tf.ones((args['output_filters'],)),name=self.outputblock.name+'_scale')
    
  def residual(self,x,input):
    if self.residual_dropout:
      x=self.residual_dropout(x)
    return tf.add(self.scale*x,input)
  @tf.function
  def call(self,input):
    if self.downsample:
        input=self.downsample_LN(input)
        input=self.downsample(input)
    x=input
    x=self.depwiseblock(x)
    x=self.LN(x)
    if self.conv_dropout:
      x=self.conv_dropout(x)
    if self.expandblock:
      x=self.expandblock(x)
    if self.se:
      x=self.se(x)
    x=self.outputblock(x)
    if self.use_residual:
      x=self.residual(x,input)
    x=self.act(x)
    return x

'''
 modified FusedMBC_Block from ConvNeXt:
    less normalization
    less activation
    Moving up depthwise conv layer. (inverted bottlenecks)
'''

class Modified_FusedMBC_Block(keras.layers.Layer):
  def __init__(self,act,args,survival_prob=1,layer_scale=1e-6):
    super().__init__()
    filters=int(args['input_filters']*args['expand_ratio'])
    self.expandblock=None
    if args['expand_ratio']!=1:
      self.expandblock=keras.layers.Conv2D(
            filters=filters,
            kernel_size=args['kernel_size'],
            strides=1,
            padding='same',
            use_bias=False,
            activation=None
          )
    self.conv_dropout=None
    if 'conv_dropout' in args.keys():
      if args['conv_dropout'] and args['expand_ratio'] > 1:
        self.conv_dropout=keras.layers.Dropout(args['conv_dropout'])
    if args['se_ratio']:
      self.se=SE(filters,args['se_ratio']/args['expand_ratio'],act)
    else:
      self.se=None
    self.outputblock=keras.layers.Conv2D(
        filters=args['output_filters'],
        kernel_size=args['kernel_size'] if args['expand_ratio'] == 1 else 1,
        strides=args['strides'] if args['expand_ratio'] == 1 else 1,
        padding='same',
        use_bias=False,
        activation=act
    )
    self.residual_dropout=None
    self.use_residual=None
    if args['strides']>1 or args['input_filters']==args['output_filters']:
      if 1-survival_prob:
        self.residual_dropout=keras.layers.SpatialDropout2D(1-survival_prob)
      self.use_residual=tf.constant(True,tf.bool)
     
    self.downsample=None
    if args['strides']>1:
        self.downsample_LN=keras.layers.LayerNormalization(epsilon=1e-6)
        self.downsample=keras.layers.Conv2D(
            filters=args['output_filters'],
            kernel_size=2,
            strides=2,
            activation=None
        )
    self.act=self.outputblock.activation
    self.outputblock.activation=None
    self.LN=keras.layers.LayerNormalization(epsilon=1e-6)
    if self.use_residual:
        self.scale=tf.Variable(layer_scale * tf.ones((args['output_filters'],)),name=self.outputblock.name+'_scale')

  def residual(self,x,input):
    if self.residual_dropout:
      x=self.residual_dropout(x)
    return tf.add(self.scale*x,input)
  @tf.function
  def call(self,input):
    if self.downsample:
        input=self.downsample_LN(input)
        input=self.downsample(input)
    x=input
    if self.expandblock:
      x=self.expandblock(x)
    x=self.LN(x)
    if self.conv_dropout:
      x=self.conv_dropout(x)
    if self.se:
      x=self.se(x)
    x=self.outputblock(x)
    x=self.act(x)
    if self.use_residual:
      x=self.residual(x,input)
    return x

class Modified_EfficientV2base(keras.Model):
  def __init__(self,act,block_cfgs,head_filters=1280,survival_prob=0.8,dropout=None,pooling='avg'):
    super().__init__()
    self.stem=keras.layers.Conv2D(
        filters=block_cfgs[0]['input_filters'],
        kernel_size=4,
        strides=2,
        padding='same',
        use_bias=False,
        activation=None,
        )
    
    self.blocks=[]
    for idx,block_cfg in enumerate(block_cfgs):
      if survival_prob:
        drop_rate = 1.0 - survival_prob
        survival_prob = 1.0 - drop_rate * float(idx) / len(block_cfgs)
      if block_cfg['conv_type']:
        self.blocks.append(Modified_FusedMBC_Block(act,block_cfg,survival_prob))
      else:
        self.blocks.append(Modified_MBC_Block(act,block_cfg,survival_prob))
    if pooling=='avg':
      self.head_pool=keras.layers.GlobalAveragePooling2D()
    elif pooling=='max':
      self.head_pool=keras.layers.GlobalMaxPool2D()
    else:
      self.head_pool=keras.layers.Flatten()
    self.dropout=None
    if dropout and dropout>0 and dropout<1:
      self.dropout=keras.layers.Dropout(dropout)    
    self.head_block=keras.layers.Conv2D(filters=head_filters,
        kernel_size=1,
        strides=1,
        padding='same',
        use_bias=False,
        activation=act)
    self.head_LN=keras.layers.LayerNormalization(epsilon=1e-6)
    
  def head(self,x):
    x=self.head_block(x)
    if self.dropout:
      x=self.dropout(x)
    x=self.head_pool(x)
    x=self.head_LN(x)
    return x
  @tf.function
  def call(self,x):
    x=self.stem(x)
    for block in self.blocks:
      x=block(x)
    x=self.head(x)
    return x
    


"""
Other configurations

# (block, width, depth, train_size, eval_size, dropout, randaug, mixup, aug)\
'efficientnetv2-s':  # 83.9% @ 22M\
    (v2_s_block, 1.0, 1.0, 300, 384, 0.2, 10, 0, 'randaug'),\
'efficientnetv2-b0':  # 78.7% @ 7M params\
    (v2_base_block, 1.0, 1.0, 192, 224, 0.2, 0, 0, 'effnetv1_autoaug'),\
'efficientnetv2-b1':  # 79.8% @ 8M params\
    (v2_base_block, 1.0, 1.1, 192, 240, 0.2, 0, 0, 'effnetv1_autoaug'),\
'efficientnetv2-b2':  # 80.5% @ 10M params\
    (v2_base_block, 1.1, 1.2, 208, 260, 0.3, 0, 0, 'effnetv1_autoaug'),\
'efficientnetv2-b3':  # 82.1% @ 14M params\
    (v2_base_block, 1.2, 1.4, 240, 300, 0.3, 0, 0, 'effnetv1_autoaug'),\

model=dict(\
        model_name='efficientnet-b0',\
        data_format='channels_last',\
        feature_size=1280,\
        bn_type=None,   # 'tpu_bn',\
        bn_momentum=0.9,\
        bn_epsilon=1e-3,\
        gn_groups=8,\
        depth_divisor=8,\
        min_depth=8,\
        act_fn='silu',\
        survival_prob=0.8,\
        local_pooling=False,\
        headbias=None,\
        conv_dropout=None,\
        dropout_rate=None,\
        depth_coefficient=None,\
        width_coefficient=None,\
        blocks_args=None,\
        num_classes=1000,  # must be the same as data.num_classes\
),
# train related params.
train=dict(\
    stages=0,\
    epochs=350,\
    min_steps=0,\
    optimizer='rmsprop',\
    lr_sched='exponential',\
    lr_base=0.016,\
    lr_decay_epoch=2.4,\
    lr_decay_factor=0.97,\
    lr_warmup_epoch=5,\
    lr_min=0,\
    ema_decay=0.9999,\
    weight_decay=1e-5,\
    weight_decay_inc=0.0,\
    weight_decay_exclude='.*(bias|gamma|beta).*',\
    label_smoothing=0.1,\
    gclip=0,\
    batch_size=4096,\
    isize=None,\
    split=None,  # dataset split, default to 'train'\
    loss_type=None,  # loss type: sigmoid or softmax\
    ft_init_ckpt=None,\
    ft_init_ema=True,\
    varsexp=None,  # trainable variables.\
    sched=None,  # schedule\
),\
eval=dict(\
    batch_size=8,\
    isize=None,  # image size\
    split=None,  # dataset split, default to 'eval'\
),
# data related params.
data=dict(\
    ds_name='imagenet',\
    augname='randaug',  # or 'autoaug'\
    ra_num_layers=2,\
    ram=15,\
    mixup_alpha=0.,\
    cutmix_alpha=0.,\
    ibase=128,\
    cache=True,\
    resize=None,\
    data_dir=None,\
    multiclass=None,\
    num_classes=1000,\
    tfds_name=None,\
    try_gcs=False,\
    tfds_split=None,\
    splits=dict(\
        train=dict(num_images=None, files=None, tfds_split=None, slice=None),\
        eval=dict(num_images=None, files=None, tfds_split=None, slice=None),\
        minival=dict(num_images=None, files=None, tfds_split=None, slice=None),\
        trainval=dict(num_images=None, files=None, tfds_split=None, slice=None),\
    ),\
),\
runtime=dict(\
    iterations_per_loop=1000,  # larger value has better utilization.\
    skip_host_call=False,\
    mixed_precision=True,\
    use_async_checkpointing=False,\
    log_step_count_steps=64,\
    keep_checkpoint_max=5,\
    keep_checkpoint_every_n_hours=5,\
    strategy='tpu',  # None, gpu, tpu\
))
"""